{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.240279Z",
     "start_time": "2025-08-05T15:32:36.707392Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "from vietnamadminunits.parser.utils import key_normalize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_DIR = Path().resolve().parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb61191932a1ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.259478Z",
     "start_time": "2025-08-05T15:32:37.252759Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_sort(text, level=1):\n",
    "#     if isinstance(text, str):\n",
    "#         if level == 1:\n",
    "#             text = re.sub(r'^Tỉnh\\s|Thành phố\\s', '', text, flags=re.IGNORECASE)\n",
    "#         elif level == 2:\n",
    "#             if re.search(r'^Quận\\s\\d{1,2}', text, flags=re.IGNORECASE):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 text = re.sub(r'^Quận\\s|Huyện\\s|Thị xã\\s|Thành phố\\s', '', text, flags=re.IGNORECASE)\n",
    "#         else:\n",
    "#             if re.search(r'^Phường\\s\\d{1,2}', text, flags=re.IGNORECASE):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 text = re.sub(r'^Phường\\s|Thị trấn\\s|Xã\\s', '', text, flags=re.IGNORECASE)\n",
    "#\n",
    "#         return text.strip()\n",
    "#     return text\n",
    "\n",
    "\n",
    "def create_sort(text, level=1):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Định nghĩa các tiền tố cần xóa theo cấp\n",
    "    REMOVE_PREFIXES = {\n",
    "        1: r'^(Tỉnh|Thành phố)\\s',\n",
    "        2: r'^(?!Quận\\s\\d{1,2})(Quận|Huyện|Thị xã|Thành phố)\\s',\n",
    "        3: r'^(?!Phường\\s\\d{1,2})(Phường|Thị trấn|Xã)\\s',\n",
    "    }\n",
    "\n",
    "    pattern = REMOVE_PREFIXES.get(level)\n",
    "    if pattern:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_keywords(row, level=1):\n",
    "    district_type_acronym = {\n",
    "        'Quận': 'q',\n",
    "        'Thị xã': 'tx',\n",
    "        'Thành phố': 'tp',\n",
    "        'Huyện': 'h',\n",
    "    }\n",
    "    ward_type_acronym = {\n",
    "        'Phường': 'p',\n",
    "        'Thị trấn': 'tt',\n",
    "        'Xã': 'x'\n",
    "    }\n",
    "\n",
    "    typing_aliases = {\n",
    "        'quy': 'qui',\n",
    "        'qui': 'quy',\n",
    "        'ngok': 'ngoc',\n",
    "        'ngoc': 'ngok',\n",
    "        'pak': 'pac',\n",
    "        'pac': 'pak',\n",
    "        'dak': 'dac',\n",
    "        'dac': 'dak',\n",
    "        'vi': 'vy',\n",
    "        'vy': 'vi',\n",
    "        'sy': 'si',\n",
    "        'si': 'sy',\n",
    "        'yang': 'jang',\n",
    "        'jang': 'yang',\n",
    "        'sa': 'xa',\n",
    "        'xa': 'sa',\n",
    "        \n",
    "    }\n",
    "\n",
    "    keywords = []\n",
    "    if level == 1:\n",
    "        keywords.append(row['provinceKey'])\n",
    "        keywords.append(row['provinceShortKey'])\n",
    "        if pd.notnull(row['provinceAlias']):\n",
    "            aliases = json.loads(row['provinceAlias'])\n",
    "            for a in aliases:\n",
    "                keywords.append(key_normalize(a))\n",
    "\n",
    "    elif level == 2:\n",
    "        keywords.append(row['districtKey'])\n",
    "\n",
    "        if not row['districtShortKeyDuplicated']:\n",
    "            keywords.append(row['districtShortKey'])\n",
    "        else:\n",
    "            keywords.append(key_normalize(f\"{row['districtShortKey']} {row['districtType']}\"))\n",
    "            keywords.append(key_normalize(f\"{district_type_acronym[row['districtType']]} {row['districtShortKey']}\"))\n",
    "\n",
    "\n",
    "        if row['districtShortDuplicated']:\n",
    "            district_type = row['districtType']\n",
    "            district_type_key = key_normalize(district_type)\n",
    "            district_type_key_acronym = district_type_acronym[row['districtType']]\n",
    "            acronym_keyword = re.sub(fr'^{district_type_key}', district_type_key_acronym, row['districtKey'])\n",
    "            keywords.append(acronym_keyword)\n",
    "\n",
    "            # Huyện Kỳ Anh, Thị xã Kỳ Anh,... thêm shortKey không có type cho type cao hơn\n",
    "            # if row['districtType'] in ['Thị xã', 'Thành phố']:\n",
    "            #     keywords.append(key_normalize(create_sort(text=row['district'], level=2)))\n",
    "            # Triển khai giải pháp tìm wardKeywords xong mới chọn mặc định\n",
    "\n",
    "        if pd.notnull(row['districtAlias']):\n",
    "            aliases = json.loads(row['districtAlias'])\n",
    "            for a in aliases:\n",
    "                keywords.append(key_normalize(a))\n",
    "\n",
    "        if re.search(r'^quan\\d{1,2}', row['districtKey'], flags=re.IGNORECASE):\n",
    "            keywords.append(row['districtKey'].replace('quan', 'q'))\n",
    "            keywords.append(row['districtKey'].replace('quan', 'district'))\n",
    "            number = row['districtKey'].replace('quan', '').zfill(2)\n",
    "            keywords.append(f\"quan{number}\")\n",
    "            keywords.append(f\"q{number}\")\n",
    "            keywords.append(f\"district{number}\")\n",
    "\n",
    "\n",
    "        match = re.search(rf\"({'|'.join(sorted(typing_aliases.keys(), key=len, reverse=True))})\", row['districtShortKey'], flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            typing_alias = match.group(0)\n",
    "            keywords.append(re.sub(fr\"{typing_alias}\", typing_aliases[typing_alias], row['districtKey'], flags=re.IGNORECASE))\n",
    "            keywords.append(re.sub(fr\"{typing_alias}\", typing_aliases[typing_alias], row['districtShortKey'], flags=re.IGNORECASE))\n",
    "\n",
    "\n",
    "    else:\n",
    "        if pd.notnull(row['wardKey']):\n",
    "            keywords.append(row['wardKey'])\n",
    "\n",
    "\n",
    "            if not row['wardShortKeyDuplicated']:\n",
    "                keywords.append(row['wardShortKey'])\n",
    "            else:\n",
    "                keywords.append(key_normalize(f\"{row['wardShortKey']} {row['wardType']}\"))\n",
    "                keywords.append(key_normalize(f\"{ward_type_acronym[row['wardType']]} {row['wardShortKey']}\"))\n",
    "\n",
    "\n",
    "            if row['wardShortDuplicated']:\n",
    "                ward_type = row['wardType']\n",
    "                ward_type_key = key_normalize(ward_type)\n",
    "                ward_type_key_acronym = ward_type_acronym[row['wardType']]\n",
    "                acronym_keyword = re.sub(fr'^{ward_type_key}', ward_type_key_acronym, row['wardKey'])\n",
    "                keywords.append(acronym_keyword)\n",
    "\n",
    "\n",
    "            if pd.notnull(row['wardAlias']):\n",
    "                aliases = json.loads(row['wardAlias'])\n",
    "                for a in aliases:\n",
    "                    # keywords.append(key_normalize(a)) # xãnhânthành và xãhợpthành cần vào xãđôngthành\n",
    "                    keywords.append(a)\n",
    "\n",
    "            if re.search(r'^phuong\\d{1,2}', row['wardKey'], flags=re.IGNORECASE):\n",
    "                keywords.append(row['wardKey'].replace('phuong', 'p'))\n",
    "                keywords.append(row['wardKey'].replace('phuong', 'f'))\n",
    "                keywords.append(row['wardKey'].replace('phuong', 'ward'))\n",
    "                number = row['wardKey'].replace('phuong', '').zfill(2)\n",
    "                keywords.append(f\"phuong{number}\")\n",
    "                keywords.append(f\"p{number}\")\n",
    "                keywords.append(f\"f{number}\")\n",
    "                keywords.append(f\"ward{number}\")\n",
    "\n",
    "            if 'thitrannongtruong' in row['wardKey']:\n",
    "                keywords.append(row['wardKey'].replace('thitrannongtruong', 'thitrannt'))\n",
    "                keywords.append(row['wardKey'].replace('thitrannongtruong', 'ttnt'))\n",
    "                keywords.append(row['wardKey'].replace('thitrannongtruong', 'nt'))\n",
    "\n",
    "\n",
    "            for key in typing_aliases.keys():\n",
    "                if key in row['wardShortKey']:\n",
    "                    keywords.append(row['wardKey'].replace(key, typing_aliases[key]))\n",
    "                    keywords.append(row['wardShortKey'].replace(key, typing_aliases[key]))\n",
    "\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    keywords = list(set(keywords))\n",
    "    keywords = sorted(keywords, key=len, reverse=True)\n",
    "    return json.dumps(keywords)\n",
    "\n",
    "\n",
    "\n",
    "def zill_code(value, level=1):\n",
    "    if not pd.isnull(value):\n",
    "        if level == 1:\n",
    "            return str(int(value)).zfill(2)\n",
    "        elif level == 2:\n",
    "            return str(int(value)).zfill(3)\n",
    "        elif level == 3:\n",
    "            return str(int(value)).zfill(5)\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da00036467b7ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.298220Z",
     "start_time": "2025-08-05T15:32:37.267685Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR / 'data/processed/legacy_63-province-10040-ward_with_location.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf17e92d0f64b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.315331Z",
     "start_time": "2025-08-05T15:32:37.305587Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f6ab9b",
   "metadata": {},
   "source": [
    "## Enriching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bdb70",
   "metadata": {},
   "source": [
    "### Adding basic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7833006a03977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.520957Z",
     "start_time": "2025-08-05T15:32:37.379621Z"
    }
   },
   "outputs": [],
   "source": [
    "unit_cols = ['province', 'district', 'ward']\n",
    "level_map = {\n",
    "    'province': 1,\n",
    "    'district': 2,\n",
    "    'ward': 3\n",
    "}\n",
    "\n",
    "for col in unit_cols:\n",
    "    # Create short version\n",
    "    level = level_map[col]\n",
    "    # df[f\"{col}Short\"] = df[col].apply(create_sort, args=(level,)) # existing\n",
    "\n",
    "    df[f\"{col}Code\"] = df[f\"{col}Code\"].apply(zill_code, args=(level,))\n",
    "\n",
    "    # Create key\n",
    "    df[f\"{col}Key\"] = df[f\"{col}\"].apply(key_normalize)\n",
    "\n",
    "    # Create short key\n",
    "    df[f\"{col}ShortKey\"] = df[f\"{col}Short\"].apply(key_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbdd23",
   "metadata": {},
   "source": [
    "### Checking duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72560416",
   "metadata": {},
   "source": [
    "#### District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faac47fc5e2a5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.747079Z",
     "start_time": "2025-08-05T15:32:37.740090Z"
    }
   },
   "outputs": [],
   "source": [
    "# districtKey\n",
    "df_district = df[['province', 'provinceKey', 'district', 'districtKey', 'districtShortKey']].drop_duplicates()\n",
    "df_district.groupby(['province', 'districtKey']).size().reset_index(name='count').sort_values(by=['count'], ascending=False).head()\n",
    "\n",
    "# Toàn bộ districtKey là unique, tuyệt vời!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d997158461ec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.783072Z",
     "start_time": "2025-08-05T15:32:37.778869Z"
    }
   },
   "outputs": [],
   "source": [
    "# districtShortKey\n",
    "count_district_short_key = df_district.groupby(['province', 'districtShortKey']).size().reset_index(name='count').sort_values(by=['count'], ascending=False)\n",
    "duplicated_district_short_key = count_district_short_key[count_district_short_key['count']>1].copy()\n",
    "duplicated_district_short_key['districtShortKeyDuplicated'] = True\n",
    "duplicated_district_short_key.drop(columns=['count'], inplace=True)\n",
    "duplicated_district_short_key\n",
    "\n",
    "# Nhờ có phần (Type) ở phía sau cho các district cùng tên nên districtShortKey không bị duplicated, ví dụ: Kỳ Anh (Huyện), Kỳ Anh (Thị xã)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da5cf9cd21001d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.910281Z",
     "start_time": "2025-08-05T15:32:37.903532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add districtShortKeyDuplicated (flow cũ khi districtShortKey vẫn có duplicated do chưa có (Type) ở phía sau cho các district cùng tên)\n",
    "# Hỗ trợ trong phần tạo keywords\n",
    "df = pd.merge(df, duplicated_district_short_key, on=['province', 'districtShortKey'], how='left')\n",
    "df['districtShortKeyDuplicated'].fillna(False, inplace=True)\n",
    "df[df['districtShortKeyDuplicated']][['province', 'district', 'districtShort']].drop_duplicates().sort_values(by='districtShort')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c609bc",
   "metadata": {},
   "source": [
    "#### Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d4d9352186aa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:37.947543Z",
     "start_time": "2025-08-05T15:32:37.939114Z"
    }
   },
   "outputs": [],
   "source": [
    "# wardKey\n",
    "count_ward_key = df.groupby(['province', 'district', 'wardKey']).size().reset_index(name='count').sort_values(by=['count'], ascending=False)\n",
    "count_ward_key['wardKeyDuplicated'] = np.where(count_ward_key['count']>1, True, False)\n",
    "duplicated_ward_key = count_ward_key[count_ward_key['wardKeyDuplicated']]\n",
    "duplicated_ward_key.drop(columns=['count'], inplace=True)\n",
    "\n",
    "print(duplicated_ward_key.shape[0])\n",
    "duplicated_ward_key\n",
    "\n",
    "# Có nhiều wardKey bị trùng trong một district vì bỏ dấu tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309164950985e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.021875Z",
     "start_time": "2025-08-05T15:32:37.987525Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add wardKeyDuplicated\n",
    "df = pd.merge(df, duplicated_ward_key, on=['province', 'district', 'wardKey'], how='left')\n",
    "df['wardKeyDuplicated'].fillna(False, inplace=True)\n",
    "\n",
    "# Đưa wardKey và wardShortKey về phiên bản có dấu tiếng Việt\n",
    "df['wardKey'] = np.where(df['wardKeyDuplicated'], df['ward'].apply(key_normalize, args=([], False)), df['wardKey'])\n",
    "df['wardShortKey'] = np.where(df['wardKeyDuplicated'], df['wardShort'].apply(key_normalize, args=([], False)), df['wardShortKey'])\n",
    "\n",
    "# Preview\n",
    "df[df['wardKeyDuplicated']][['province', 'district', 'ward']].sort_values(by=['province', 'district', 'ward']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6178c11bc2ff3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.075786Z",
     "start_time": "2025-08-05T15:32:38.067476Z"
    }
   },
   "outputs": [],
   "source": [
    "# wardShortKey\n",
    "count_ward_short_key = df.groupby(['province', 'district', 'wardShortKey']).size().reset_index(name='count').sort_values(by=['count'], ascending=False)\n",
    "duplicated_ward_short_key = count_ward_short_key[count_ward_short_key['count']>1].copy()\n",
    "duplicated_ward_short_key['wardShortKeyDuplicated'] = True\n",
    "duplicated_ward_short_key.drop(columns=['count'], inplace=True)\n",
    "duplicated_ward_short_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b1c4ae4125f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.141026Z",
     "start_time": "2025-08-05T15:32:38.130523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add wardShortKeyDuplicated\n",
    "df = pd.merge(df, duplicated_ward_short_key, on=['province', 'district', 'wardShortKey'], how='left')\n",
    "df['wardShortKeyDuplicated'].fillna(False, inplace=True)\n",
    "\n",
    "# Preview\n",
    "df[df['wardShortKeyDuplicated']][['province', 'district', 'ward']]\n",
    "\n",
    "# Cần thêm một DICT mà wardKey là no-accented nhưng wardKeyShort là accented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359771f",
   "metadata": {},
   "source": [
    "## Creating list of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CREATE ALIAS, phải làm sau khi đã fix duplicate keys, ví dụ như xãđôngthành\n",
    "\n",
    "# Create alias columns with nan value\n",
    "for col in ['province', 'district', 'ward']:\n",
    "    df[f\"{col}Alias\"] = np.nan\n",
    "\n",
    "df_province_alias = pd.read_csv(BASE_DIR / 'data/alias_keywords/legacy/alias_province.csv')\n",
    "df_district_alias = pd.read_csv(BASE_DIR / 'data/alias_keywords/legacy/alias_district.csv')\n",
    "df_ward_alias = pd.read_csv(BASE_DIR / 'data/alias_keywords/legacy/alias_ward.csv')\n",
    "\n",
    "province_alias_map = (\n",
    "    df_province_alias\n",
    "    .groupby('province_key')['alias_keyword']\n",
    "    .apply(list)\n",
    "    .apply(json.dumps)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df['provinceAlias'] = df['provinceKey'].map(province_alias_map)\n",
    "\n",
    "\n",
    "district_alias_map = (\n",
    "    df_district_alias\n",
    "    .groupby(['province_key', 'district_key'])['alias_keyword']\n",
    "    .apply(list)\n",
    "    .apply(json.dumps)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df['districtAlias'] = df.apply(\n",
    "    lambda row: district_alias_map.get((row['provinceKey'], row['districtKey'])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "ward_alias_map = (\n",
    "    df_ward_alias\n",
    "    .groupby(['province_key', 'district_key', 'ward_key'])['alias_keyword']\n",
    "    .apply(list)\n",
    "    .apply(json.dumps)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df['wardAlias'] = df.apply(\n",
    "    lambda row: ward_alias_map.get((row['provinceKey'], row['districtKey'], row['wardKey'])),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6eb7d9e325cc71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.490725Z",
     "start_time": "2025-08-05T15:32:38.207029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create keywords\n",
    "for col in unit_cols:\n",
    "    level = level_map[col]\n",
    "    df[f\"{col}Keywords\"] = df.apply(lambda row: create_keywords(row, level=level), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e1b458",
   "metadata": {},
   "source": [
    "## Creating dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468514bf3b53a00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.610059Z",
     "start_time": "2025-08-05T15:32:38.511351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Province map\n",
    "df_province = df[['provinceKey', 'provinceKeywords', 'province', 'provinceShort', 'provinceLat', 'provinceLon', 'provinceCode']].drop_duplicates().reset_index(drop=True)\n",
    "DICT_PROVINCE = {}\n",
    "for _, row in df_province.iterrows():\n",
    "    DICT_PROVINCE[row['provinceKey']] = {\n",
    "        'provinceKeywords': json.loads(row['provinceKeywords']),\n",
    "        'province': row['province'],\n",
    "        'provinceShort': row['provinceShort'],\n",
    "        'provinceLat': row['provinceLat'],\n",
    "        'provinceLon': row['provinceLon'],\n",
    "        'provinceCode': row['provinceCode'],\n",
    "    }\n",
    "\n",
    "\n",
    "# District map\n",
    "df_district = df[['provinceKey', 'provinceShortKey', 'districtKey', 'districtShortKey', 'districtKeywords', 'district', 'districtType', 'districtShort', 'districtLat', 'districtLon', 'districtCode']].drop_duplicates().reset_index(drop=True)\n",
    "DICT_PROVINCE_DISTRICT = {}\n",
    "for _, province_row in df_province.iterrows():\n",
    "    province_key = province_row['provinceKey']\n",
    "    DICT_PROVINCE_DISTRICT[province_key] = {}\n",
    "\n",
    "    df_district_filtered = df_district[df_district['provinceKey'] == province_key]\n",
    "\n",
    "    for _, district_row in df_district_filtered.iterrows():\n",
    "        DICT_PROVINCE_DISTRICT[province_key][district_row['districtKey']] = {\n",
    "            'districtKeywords': json.loads(district_row['districtKeywords']) if pd.notnull(district_row['districtKeywords']) else [],\n",
    "            'district': district_row['district'],\n",
    "            'districtType': district_row['districtType'],\n",
    "            'districtShort': district_row['districtShort'],\n",
    "            'districtLat': district_row['districtLat'],\n",
    "            'districtLon': district_row['districtLon'],\n",
    "            'districtCode': district_row['districtCode'],\n",
    "        }\n",
    "\n",
    "\n",
    "# Unique district to province map\n",
    "province_short_keys = df['provinceShortKey'].unique().tolist()\n",
    "for index, row in df_district.iterrows():\n",
    "    district_short_key = row['districtShortKey']\n",
    "    left_district_short_keys = df_district.loc[df_district.index != index, 'districtShortKey'].tolist()\n",
    "    if district_short_key not in province_short_keys and district_short_key not in left_district_short_keys:\n",
    "        df_district.loc[index, 'districtUnique'] = True\n",
    "df_district['districtUnique'].fillna(False, inplace=True)\n",
    "df_district_unique = df_district[df_district['districtUnique']==True]\n",
    "\n",
    "DICT_UNIQUE_DISTRICT_PROVINCE = {}\n",
    "for _, row in df_district_unique.iterrows():\n",
    "    DICT_UNIQUE_DISTRICT_PROVINCE[row['districtKey']] = {\n",
    "        'districtKeywords': json.loads(row['districtKeywords']),\n",
    "        'provinceKey': row['provinceKey']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47236ee85acb4c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:38.824522Z",
     "start_time": "2025-08-05T15:32:38.631207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ward map\n",
    "df_ward = df[['provinceKey', 'districtKey', 'wardKey', 'wardKeywords', 'ward', 'wardShort', 'wardType', 'wardKeyDuplicated', 'wardLat', 'wardLon', 'wardCode']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df_ward_no_accented = df_ward[df_ward['wardKeyDuplicated']==False]\n",
    "df_ward_accented = df_ward[df_ward['wardKeyDuplicated']==True]\n",
    "\n",
    "def build_province_district_ward_dict(df, short_name_key=False):\n",
    "    DICT_PROVINCE_DISTRICT_WARD = {}\n",
    "\n",
    "    for province_key, province_group in df.groupby('provinceKey'):\n",
    "        DICT_PROVINCE_DISTRICT_WARD[province_key] = {}\n",
    "\n",
    "        for district_key, district_group in province_group.groupby('districtKey'):\n",
    "            DICT_PROVINCE_DISTRICT_WARD[province_key][district_key] = {}\n",
    "\n",
    "            for _, row in district_group.iterrows():\n",
    "                ward_key = row['wardKey']\n",
    "                if short_name_key:\n",
    "                    keywords = [key_normalize(row['wardShort'], decode=False)]\n",
    "                else:\n",
    "                    keywords = json.loads(row['wardKeywords']) if pd.notnull(row['wardKeywords']) else []\n",
    "                DICT_PROVINCE_DISTRICT_WARD[province_key][district_key][ward_key] = {\n",
    "                    'wardKeywords': keywords,\n",
    "                    'ward': row['ward'],\n",
    "                    'wardShort': row['wardShort'],\n",
    "                    'wardType': row['wardType'],\n",
    "                    'wardLat': row['wardLat'],\n",
    "                    'wardLon': row['wardLon'],\n",
    "                    'wardCode': row['wardCode'],\n",
    "                }\n",
    "\n",
    "    return DICT_PROVINCE_DISTRICT_WARD\n",
    "\n",
    "\n",
    "DICT_PROVINCE_DISTRICT_WARD_NO_ACCENTED = build_province_district_ward_dict(df_ward_no_accented)\n",
    "DICT_PROVINCE_DISTRICT_WARD_ACCENTED = build_province_district_ward_dict(df_ward_accented)\n",
    "\n",
    "df_ward_short_accented = df[df['wardShortKeyDuplicated']]\n",
    "DICT_PROVINCE_DISTRICT_WARD_SHORT_ACCENTED = build_province_district_ward_dict(df_ward_short_accented, short_name_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9419f0e1b85a6f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:39.025019Z",
     "start_time": "2025-08-05T15:32:39.010942Z"
    }
   },
   "outputs": [],
   "source": [
    "df_district_divided = pd.read_csv(BASE_DIR / 'data/alias_keywords/legacy/divided_district.csv')\n",
    "df_district_divided['districtDefault'].fillna(False, inplace=True)\n",
    "\n",
    "# Khởi tạo dict thường\n",
    "DICT_PROVINCE_DISTRICT_DIVIDED = {}\n",
    "\n",
    "# Duyệt qua từng dòng trong bảng chia tách\n",
    "for _, row in df_district_divided.iterrows():\n",
    "    province_key = row['provinceKey']\n",
    "    divided_key = row['dividedDistrictKey']\n",
    "    divided_keywords = json.loads(row['dividedDistrictKeyWords']) if isinstance(row['dividedDistrictKeyWords'], str) else []\n",
    "    district_key = row['districtKey']\n",
    "    is_default = bool(row['districtDefault'])\n",
    "\n",
    "    # Lấy ward keywords\n",
    "    mask = (df['provinceKey'] == province_key) & (df['districtKey'] == district_key)\n",
    "    ward_keywords = df.loc[mask, 'wardKeywords'].dropna().tolist()\n",
    "    ward_keywords_flat = sum([json.loads(w) if isinstance(w, str) else [] for w in ward_keywords], [])\n",
    "\n",
    "    # Tạo các cấp nếu chưa có\n",
    "    if province_key not in DICT_PROVINCE_DISTRICT_DIVIDED:\n",
    "        DICT_PROVINCE_DISTRICT_DIVIDED[province_key] = {}\n",
    "    if divided_key not in DICT_PROVINCE_DISTRICT_DIVIDED[province_key]:\n",
    "        DICT_PROVINCE_DISTRICT_DIVIDED[province_key][divided_key] = {\n",
    "            'dividedDistrictKeywords': divided_keywords,\n",
    "            'districts': {}\n",
    "        }\n",
    "\n",
    "    # Gán dữ liệu district\n",
    "    DICT_PROVINCE_DISTRICT_DIVIDED[province_key][divided_key]['districts'][district_key] = {\n",
    "        'wardKeywords': ward_keywords_flat,\n",
    "        'districtDefault': is_default\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ade0c",
   "metadata": {},
   "source": [
    "## Saving package data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bba4c4a137610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:39.064996Z",
     "start_time": "2025-08-05T15:32:39.063505Z"
    }
   },
   "outputs": [],
   "source": [
    "# DICT\n",
    "parser_data = {\n",
    "    'DICT_PROVINCE': DICT_PROVINCE,\n",
    "    'DICT_PROVINCE_DISTRICT': DICT_PROVINCE_DISTRICT,\n",
    "    'DICT_UNIQUE_DISTRICT_PROVINCE': DICT_UNIQUE_DISTRICT_PROVINCE,\n",
    "    'DICT_PROVINCE_DISTRICT_WARD_NO_ACCENTED': DICT_PROVINCE_DISTRICT_WARD_NO_ACCENTED,\n",
    "    'DICT_PROVINCE_DISTRICT_WARD_ACCENTED': DICT_PROVINCE_DISTRICT_WARD_ACCENTED,\n",
    "    'DICT_PROVINCE_DISTRICT_WARD_SHORT_ACCENTED': DICT_PROVINCE_DISTRICT_WARD_SHORT_ACCENTED,\n",
    "    'DICT_PROVINCE_DISTRICT_DIVIDED': DICT_PROVINCE_DISTRICT_DIVIDED\n",
    "}\n",
    "\n",
    "with open(BASE_DIR / 'vietnamadminunits/data/parser_legacy.json', 'w') as f:\n",
    "    json.dump(parser_data, f)\n",
    "\n",
    "# SQLite\n",
    "import sqlite3\n",
    "with sqlite3.connect(BASE_DIR / 'vietnamadminunits/data/dataset.db') as conn:\n",
    "    df.to_sql('admin_units_legacy', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc3650cbee6d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:39.214789Z",
     "start_time": "2025-08-05T15:32:39.161980Z"
    }
   },
   "source": [
    "## Saving interim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190e48980d36fcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T15:32:39.304070Z",
     "start_time": "2025-08-05T15:32:39.231483Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(BASE_DIR / 'data/interim/legacy_63-province-10040-ward_with_location_and_key.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
